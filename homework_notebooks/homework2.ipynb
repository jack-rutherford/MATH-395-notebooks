{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Gradient Descent (into madness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>\n",
    "\n",
    "Consider the three-hump Camel function used in class $$f(x1, x2) = 2x_1^2 − 1.05x^4_1 + \\frac{1}{6}x^6_1 + x_1x_2 + x_2^2$$ Its gradient is $$\\nabla f = [4x_1 − 4.2x^3_1 + x^5_1 + x_2, x_1 + 2x_2]^T$$ \n",
    "\n",
    "Do the following:\n",
    "    </li>\n",
    "        <ol type='a'>\n",
    "            <li>\n",
    "                Perform gradient descent with initial point $x_0 = (0.8, −0.5)$ and a variable learning rate $$x_t = x_{t-1} − \\eta _{t−1} · \\nabla f(x_{t−1}), t ≥ 1$$ where\n",
    "                $$\\eta _t = \\beta · \\eta _{t−1}, t ≥ 1, \\eta _0 = 0.5$$\n",
    "                Here, $0 < \\beta < 1$ is the shrinking factor such that η is reduce exactly by half after every five\n",
    "                iterations. Does gradient descent converge to some point in this case? If yes, where does it\n",
    "                converge to?\n",
    "            </li>\n",
    "            <li>\n",
    "                Select 100 distinct initial locations within the rectangular region $[−3, 3] × [−2, 2]$, which can\n",
    "                be either a uniform grid over the region or a random set of points sampled from the region.\n",
    "                For each choice of the initial point, perform gradient descent with the above variable learning\n",
    "                rate schedule. How many of them converged, and where did they converge to? Display the\n",
    "                100 points on top of the level set, color coded by which local minimum they converged to.\n",
    "            </li>\n",
    "            <li>\n",
    "                In https://www.ceremade.dauphine.fr/~waldspurger/tds/22_23_s1/advanced_gradient_descent.pdf, \n",
    "                a variant of gradient descent, called heavy ball, is introduced (see Algorithm 2\n",
    "                on page 7; the parameter α there is our $\\eta$). Implement the heavy ball scheme on the above\n",
    "                function with $x_0 = (0.8, −0.5), \\alpha = 0.5$ and different values of the momentum parameter $\\gamma$ = 0\n",
    "                (no momentum), $0.1, 0.2, . . . , 0.9$ and $1$ (full momentum). Display a few typical trajectories of\n",
    "                gradient descent. Which values of $\\gamma$ led to convergence to the local minimum at (0, 0)?\n",
    "            </li>\n",
    "        </o1>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol start=2>\n",
    "    <li>\n",
    "    \n",
    "Use the following code to simulate a linear system of $n$ equations with $m$ unknowns:\n",
    "\n",
    "$n = 20$<br>\n",
    "$m = 10$<br>\n",
    "$A = np.random.normal(0, 1, size = (n, m))$<br>\n",
    "$x\\_true = np.random.normal(0, 1, size = (m, 1))$<br>\n",
    "$b = A.dot(x\\_true) + np.random.normal(0, 0.1, size = (n, 1))$<br>\n",
    "Consider the corresponding least squares problem\n",
    "$$\\underset{x}{min}\\frac{1}{n}\\lvert\\lvert Ax-b \\rvert\\rvert^2$$\n",
    "Here, a factor of $\\frac{1}{n}$ is incorporated to the objective function. It does not change the minimizer but\n",
    "will help control the magnitude of the gradient.\n",
    "\n",
    "Solve the above problem using two different methods:\n",
    "    </li>\n",
    "        <ol type=\"a\">\n",
    "            <li>\n",
    "                The analytical method, by solving the linear system $(A^T A)x = A^Tb$;\n",
    "            </li>\n",
    "            <li>\n",
    "                Gradient descent (with proper values of the learning rate and number of iterations ). Plot the x-convergence curve.\n",
    "            </li>\n",
    "        </ol>\n",
    "        \n",
    "Do the two methods give you (nearly) the same solution?\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol start=3>\n",
    "    <li>\n",
    "    \n",
    "Repeat part (b) with the following values of $n = 200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000, 10000$\n",
    "        and $m = \\frac{n}{2}$ (for each fixed value of $n$). Use the code below to record and then plot the running\n",
    "        times for both of the methods (exact or gradient descent) against $n$. You may want to use log scale\n",
    "        for one or both quantities (running time, $n$).\n",
    "        <br><br>$\\text{import time}$<br>\n",
    "        $\\text{start time = time.time()}$<br> \n",
    "        $\\text{\\#Steps used by the exact method or gradient descent}$<br>\n",
    "        $\\text{runtime = time.time() - start time}$<br>\n",
    "    </li>\n",
    "</ol>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
